# Sample YAML configuration file for Remembrances-MCP
# This file contains all available configuration options with their default values.
# You can copy this file to config.yaml and modify the values as needed.
#
# Standard locations (auto-detected if --config not specified):
#   Linux: ~/.config/remembrances/config.yaml
#   macOS: ~/Library/Application Support/remembrances/config.yaml
#
# Configuration can be overridden by command-line flags or environment variables.
# Environment variables use the GOMEM_ prefix (e.g., GOMEM_SSE_ADDR).
# Command-line flags take precedence over YAML, and environment variables over both.

# Enable SSE transport (default: false)
#sse: false

# Address to bind SSE transport (host:port) (default: ":3000")
#sse-addr: "3000"

# Enable HTTP JSON API transport (default: false)
#http: false

# Address to bind HTTP transport (host:port) (default: ":8080")
#http-addr: "8080"

# Enable REST API server (default: false)
#rest-api-serve: false

# Path to the knowledge base directory (default: "")
knowledge-base: "/www/MCP/remembrances-mcp/.serena/memories"

# ========== SurrealDB Configuration ==========
# Path to the embedded SurrealDB database (default: "./remembrances.db")
db-path: "surrealkv:///www/MCP/remembrances-mcp/remembrances.db"

# URL for the remote SurrealDB instance (default: "")
#surrealdb-url: "ws://localhost:8000"

# Username for SurrealDB (default: "root")
surrealdb-user: "root"

# Password for SurrealDB (default: "root")
surrealdb-pass: "root"

# Namespace for SurrealDB (default: "test")
surrealdb-namespace: "test"

# Database for SurrealDB (default: "test")
surrealdb-database: "test"

# External command to start SurrealDB when connection fails (default: "")
#surrealdb-start-cmd: "surreal start --user root --pass root surrealkv:///www/Remembrances/programming"

# ========== GGUF Local Model Configuration ==========
# Path to GGUF model file for local embeddings (default: "")
# When set, this takes priority over Ollama and OpenAI
# Example: "/path/to/nomic-embed-text-v1.5.Q4_K_M.gguf"
gguf-model-path: "/www/Remembrances/nomic-embed-text-v1.5.Q4_K_M.gguf"

# Number of threads for GGUF model (0 = auto-detect) (default: 0)
#gguf-threads: 0

# Number of GPU layers for GGUF model (0 = CPU only) (default: 0)
# Higher values offload more computation to GPU
gguf-gpu-layers: 32
# ========== Ollama Configuration ==========
# URL for the Ollama server (default: "http://localhost:11434")
#ollama-url: "http://localhost:11434"

# Ollama model to use for embeddings (default: "")
#ollama-model: "nomic-embed-text:latest"
# ========== OpenAI Configuration ==========
# OpenAI API key (default: "")
#openai-key: ""

# OpenAI base URL (default: "https://api.openai.com/v1")
#openai-url: "https://api.openai.com/v1"

# OpenAI model to use for embeddings (default: "text-embedding-3-large")
#openai-model: "text-embedding-3-large"

# ========== Code-Specific Embedding Configuration ==========
# These options allow using specialized code embedding models for code indexing
# while using a different model for text/facts/vectors/events.
# If not specified, the default GGUF model above is used for code indexing.
#
# Recommended code embedding models:
#   - GGUF: CodeRankEmbed (coderankembed.Q4_K_M.gguf) - optimized for code
#   - Ollama: jina/jina-embeddings-v2-base-code
#
# Example: Use CodeRankEmbed for code and nomic-embed-text for general text:
#code-gguf-model-path: "/www/Remembrances/coderankembed.Q4_K_M.gguf"

# Path to GGUF model for code embeddings (default: uses default gguf-model-path)
code-gguf-model-path: "/www/Remembrances/coderankembed.Q4_K_M.gguf"

# Ollama model for code embeddings (default: uses default ollama-model)
#code-ollama-model: ""

# OpenAI model for code embeddings (default: uses default openai-model)
#code-openai-model: ""

# ========== Text Chunking Configuration ==========
# Maximum chunk size in characters for text splitting (default: 1500)
# This applies to all embedding providers (GGUF, Ollama, OpenAI)
# Larger values mean fewer chunks but may exceed model context limits
#chunk-size: 1500

# Overlap between chunks in characters (default: 200)
# Overlap helps preserve context across chunk boundaries
# Typical values are 10-20% of chunk-size
#chunk-overlap: 200

# ========== Logging Configuration ==========
# Path to the log file (logs will be written to both stdout and file) (default: "")
log: "/www/MCP/remembrances-mcp/remembrances-mcp.log"
