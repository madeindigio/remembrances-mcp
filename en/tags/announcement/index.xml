<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Announcement on Remembrances</title><link>https://madeindigio.github.io/remembrances-mcp/en/tags/announcement/</link><description>Recent content in Announcement on Remembrances</description><generator>Hugo</generator><language>en</language><lastBuildDate>Tue, 18 Nov 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://madeindigio.github.io/remembrances-mcp/en/tags/announcement/index.xml" rel="self" type="application/rss+xml"/><item><title>Release 1.0.0: True Local-First AI Memory</title><link>https://madeindigio.github.io/remembrances-mcp/en/blog/2025/release-1.0.0/</link><pubDate>Tue, 18 Nov 2025 00:00:00 +0000</pubDate><guid>https://madeindigio.github.io/remembrances-mcp/en/blog/2025/release-1.0.0/</guid><description>&lt;p>We&amp;rsquo;re thrilled to announce the release of &lt;strong>Remembrances MCP 1.0.0&lt;/strong> â€“ a major milestone that delivers on our promise of truly local-first AI memory!&lt;/p>
&lt;h2 id="whats-new">What&amp;rsquo;s New&lt;/h2>
&lt;h3 id="-native-gguf-model-support">ðŸ§  Native GGUF Model Support&lt;/h3>
&lt;p>The headline feature of this release is &lt;strong>built-in support for GGUF embedding models&lt;/strong>. You no longer need to run Ollama or rely on external OpenAI-compatible APIs to generate embeddings. Simply download a GGUF model from Hugging Face and point Remembrances MCP to it:&lt;/p></description></item></channel></rss>