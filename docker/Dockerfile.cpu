# Dockerfile for Remembrances-MCP - CPU Version (Lightweight)
# This Dockerfile is designed for LOCAL builds where the binary and libraries
# are pre-compiled on the host machine (CPU-only, no CUDA).
#
# Build: make docker-build-cpu
# Run:   make docker-run-cpu
#
# For manual build:
#   docker build -f docker/Dockerfile.cpu -t ghcr.io/madeindigio/remembrances-mcp:cpu .
#
# For manual run:
#   docker run -d \
#     -p 8080:8080 \
#     -v /path/to/data:/data \
#     -v /path/to/knowledge-base:/knowledge-base \
#     ghcr.io/madeindigio/remembrances-mcp:cpu
#
# Environment variables for runtime configuration:
#   GOMEM_HTTP_ADDR       - HTTP API address (default: :8080)
#   GOMEM_GGUF_THREADS    - CPU threads for GGUF model (default: 0 = auto)
#   GOMEM_SURREALDB_USER  - SurrealDB username (default: root)
#   GOMEM_SURREALDB_PASS  - SurrealDB password (default: root)

# Use lightweight Debian base for minimal image size (~100MB base)
FROM debian:bookworm-slim

# Build arguments for versioning and paths
ARG VERSION=dev
ARG COMMIT=unknown
ARG DIST_DIR=dist-variants/remembrances-mcp-cpu-linux-x86_64

LABEL org.opencontainers.image.source="https://github.com/madeindigio/remembrances-mcp"
LABEL org.opencontainers.image.description="Remembrances-MCP Server - CPU version (lightweight)"
LABEL org.opencontainers.image.licenses="MIT"
LABEL org.opencontainers.image.version="${VERSION}"
LABEL org.opencontainers.image.revision="${COMMIT}"
LABEL maintainer="madeindigio"

# Install minimal runtime dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Create non-root user for security
RUN groupadd -r remembrances && useradd -r -g remembrances remembrances

# Create directories for data, config, models, and knowledge base
RUN mkdir -p /app /data /config /models /knowledge-base \
    && chown -R remembrances:remembrances /app /data /config /models /knowledge-base

WORKDIR /app

# Copy pre-built binary and shared libraries from local build
# These must be built locally using: make dist-variant VARIANT=cpu
# DIST_DIR is passed as build arg from Makefile
COPY --chown=remembrances:remembrances ${DIST_DIR}/remembrances-mcp /app/
COPY --chown=remembrances:remembrances ${DIST_DIR}/*.so* /app/

# Copy GGUF model (downloaded separately or provided)
# Model: nomic-embed-text-v1.5.Q4_K_M.gguf (~260MB)
# Download from: https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q4_K_M.gguf
COPY --chown=remembrances:remembrances models/nomic-embed-text-v1.5.Q4_K_M.gguf /models/

# Copy sample configuration for reference
COPY --chown=remembrances:remembrances config.sample.gguf.yaml /config/config.sample.yaml

# Create default configuration for Docker
# Note: Environment variables (GOMEM_*) can override these settings at runtime
RUN cat > /config/config.yaml << 'EOF'
# Remembrances-MCP Docker Configuration (CPU Version)
# This configuration is optimized for containerized deployment
#
# Environment variables (GOMEM_*) take precedence over these values.
# Example: GOMEM_GGUF_THREADS=8 will override gguf-threads below.

# Enable HTTP API transport
http: true
http-addr: ":8080"

# Path to the knowledge base directory (mount as volume)
knowledge-base: "/knowledge-base"

# ========== SurrealDB Configuration ==========
# Path to the embedded SurrealDB database (mount as volume for persistence)
db-path: "surrealkv:///data/remembrances.db"

# SurrealDB credentials
surrealdb-user: "root"
surrealdb-pass: "root"
surrealdb-namespace: "remembrances"
surrealdb-database: "production"

# ========== GGUF Local Model Configuration ==========
# Path to GGUF model file for local embeddings
gguf-model-path: "/models/nomic-embed-text-v1.5.Q4_K_M.gguf"

# Number of threads for GGUF model (0 = auto-detect)
# For CPU-only, adjust based on available cores
gguf-threads: 0

# Number of GPU layers - set to 0 for CPU-only
gguf-gpu-layers: 0

# ========== Text Chunking Configuration ==========
chunk-size: 1500
chunk-overlap: 200
EOF

# Set library path for runtime - required for llama.cpp libraries
ENV LD_LIBRARY_PATH="/app"

# Default environment variables (can be overridden at runtime)
# These use the GOMEM_ prefix as expected by the application
ENV GOMEM_HTTP="true"
ENV GOMEM_HTTP_ADDR=":8080"
ENV GOMEM_DB_PATH="surrealkv:///data/remembrances.db"
ENV GOMEM_KNOWLEDGE_BASE="/knowledge-base"
ENV GOMEM_GGUF_MODEL_PATH="/models/nomic-embed-text-v1.5.Q4_K_M.gguf"
ENV GOMEM_GGUF_GPU_LAYERS="0"
ENV GOMEM_GGUF_THREADS="0"

# Make binary executable
RUN chmod +x /app/remembrances-mcp

# Switch to non-root user
USER remembrances

# Expose HTTP API port
# Use -p HOST_PORT:8080 when running to bind to a different port
EXPOSE 8080

# Health check - verify the HTTP API is responding
HEALTHCHECK --interval=30s --timeout=10s --start-period=10s --retries=3 \
    CMD curl -sf http://localhost:8080/health || exit 1

# Volume mount points for persistent data
# - /data: SurrealDB database storage (REQUIRED for persistence)
# - /knowledge-base: Knowledge base documents (REQUIRED for persistence)
# - /models: GGUF models (OPTIONAL, built-in model included)
# - /config: Configuration files (OPTIONAL, built-in config included)
VOLUME ["/data", "/knowledge-base"]

# Default command - use config file
# Override with: docker run ... ghcr.io/madeindigio/remembrances-mcp:cpu --sse --sse-addr :3000
ENTRYPOINT ["/app/remembrances-mcp"]
CMD ["--config", "/config/config.yaml"]
